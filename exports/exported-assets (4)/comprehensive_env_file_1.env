# ================================================================================================
# OP TRADING PLATFORM - COMPREHENSIVE ENVIRONMENT CONFIGURATION FILE
# Generated: 2025-08-25 11:40 AM IST
# Version: 1.0.0 - Complete Production Ready Configuration
# 
# This file contains ALL configuration variables for the OP Trading Platform
# Includes detailed comments, criteria, and procedures for each setting
# ================================================================================================

# ================================
# DEPLOYMENT CONFIGURATION
# ================================

# Deployment mode - determines service behavior and resource allocation
# first_time: Basic setup with minimal resources
# development: Debug enabled, hot reload, verbose logging
# production: Optimized performance, security hardened
DEPLOYMENT_MODE=production

# Environment identifier - used for service discovery and logging
# Allowed values: development, staging, production, testing
ENV=production

# Application version for deployment tracking
VERSION=1.0.0

# Debug mode - enables detailed logging and development tools
# CRITICAL: Must be false in production for security and performance
# Set to true only in development environment
DEBUG=false

# Logging level - controls verbosity of application logs
# DEBUG: All messages (use only in development)
# INFO: General information messages (recommended for production)
# WARNING: Warning messages and above
# ERROR: Error messages only
# CRITICAL: Critical errors only
LOG_LEVEL=INFO

# Enable structured JSON logging for better log processing
# true: Logs in JSON format with structured fields (recommended)
# false: Traditional text-based logging
ENABLE_STRUCTURED_LOGGING=true

# Log format type
# json: Structured JSON logs (recommended for production)
# text: Human-readable text logs (better for development)
LOG_FORMAT=json

# Include correlation IDs for request tracing
INCLUDE_TRACE_ID=true
INCLUDE_REQUEST_ID=true
INCLUDE_USER_ID=true

# Additional logging context
LOG_INCLUDE_HOSTNAME=true
LOG_INCLUDE_PROCESS_ID=true

# ================================
# DATA SOURCE CONFIGURATION
# ================================

# Data source mode - determines where market data comes from
# live: Real-time data from Kite Connect API (requires valid credentials)
# mock: Simulated data for testing and development
# hybrid: Live data when available, mock as fallback
DATA_SOURCE_MODE=live

# Enable mock data for testing scenarios
# true: Use simulated market data (safe for development)
# false: Use real market data (requires Kite Connect)
MOCK_DATA_ENABLED=false

# Mock data volatility - controls randomness in simulated data
# 0.1: Low volatility (minimal price changes)
# 0.2: Normal volatility (realistic market movements)
# 0.5: High volatility (extreme price swings for stress testing)
MOCK_DATA_VOLATILITY=0.2

# ================================
# MEMORY MAPPING CONFIGURATION
# ================================

# Enable memory mapping for file access optimization
# CRITERIA FOR ENABLING:
# - System has 8GB+ RAM
# - Large data files (>10MB each)
# - Frequent file access patterns
# - SSD storage (recommended)
# 
# CRITERIA FOR DISABLING:
# - System has <4GB RAM
# - Small data files (<1MB each)
# - Limited memory availability
# - HDD storage (mechanical drives)
USE_MEMORY_MAPPING=true

# Memory mapping cache size in MB
# Small systems (4GB RAM): 256
# Medium systems (8GB RAM): 512
# Large systems (16GB+ RAM): 1024
MEMORY_MAPPING_CACHE_SIZE_MB=512

# Memory mapping file threshold - minimum file size to use memory mapping
# Files smaller than this will use regular file I/O
MEMORY_MAPPING_MIN_FILE_SIZE_MB=5

# ================================
# COMPRESSION CONFIGURATION
# ================================

# Enable data compression for storage efficiency
# WHEN TO ENABLE:
# - Limited disk space
# - Network transfer optimization
# - Long-term data archival
# - CPU resources are abundant
#
# WHEN TO DISABLE:
# - Real-time processing requirements
# - CPU-constrained systems
# - Temporary/short-lived data
COMPRESSION_ENABLED=true

# Compression level (1-9)
# Level 1: Fastest compression, larger files (real-time scenarios)
# Level 3: Balanced speed and compression (recommended for live data)
# Level 6: Good compression, moderate CPU (recommended for most cases)
# Level 9: Maximum compression, slower processing (archival storage)
#
# SELECTION CRITERIA:
# - Real-time data: Level 1-3
# - Batch processing: Level 6
# - Archive storage: Level 9
COMPRESSION_LEVEL=6

# Compression algorithm
# gzip: Standard compression (good compatibility)
# lz4: Fast compression (better for real-time)
# zstd: Modern compression (best balance)
COMPRESSION_ALGORITHM=gzip

# Enable compression for different data types
COMPRESS_CSV_FILES=true
COMPRESS_JSON_FILES=true
COMPRESS_LOG_FILES=true

# ================================
# BUFFER SIZE CONFIGURATION
# ================================

# CSV buffer size in bytes - controls memory usage vs write frequency
# SIZING GUIDELINES:
# Systems with <4GB RAM: 4096 (4KB)
# Systems with 4-8GB RAM: 8192 (8KB) - RECOMMENDED
# Systems with 8-16GB RAM: 16384 (16KB)
# Systems with 16GB+ RAM: 32768 (32KB)
#
# TRADE-OFFS:
# Larger buffers: Fewer I/O operations, more memory usage
# Smaller buffers: More I/O operations, less memory usage
CSV_BUFFER_SIZE=8192

# JSON buffer size - typically 2x CSV buffer size due to JSON overhead
# JSON files are generally larger and more complex than CSV
JSON_BUFFER_SIZE=16384

# Write buffer flush interval in seconds
# Determines how often buffers are flushed to disk
# Lower values: More frequent writes, better data safety
# Higher values: Better performance, risk of data loss on crash
BUFFER_FLUSH_INTERVAL_SECONDS=30

# Maximum buffer memory usage before forced flush
# Prevents excessive memory consumption
MAX_BUFFER_MEMORY_MB=64

# ================================
# MEMORY USAGE LIMITS
# ================================

# Maximum memory usage limit in MB - SOFT LIMIT for application guidance
# IMPORTANT: This is NOT a hard limit - system won't crash if exceeded
# Application uses this for:
# - Memory cleanup triggers
# - Resource allocation decisions  
# - Performance optimization
# - Warning thresholds
#
# SIZING BY SYSTEM CAPACITY:
# 4GB system: 1024 MB (25% of total)
# 8GB system: 2048 MB (25% of total) - RECOMMENDED
# 16GB system: 4096 MB (25% of total)  
# 32GB+ system: 8192+ MB (25% of total)
#
# WHY THIS WON'T DISRUPT YOUR SYSTEM:
# - Operating system manages memory automatically
# - Virtual memory provides overflow protection
# - Other applications can still use remaining RAM
# - Application monitors and adapts its behavior
MAX_MEMORY_USAGE_MB=2048

# Memory usage warning threshold (% of MAX_MEMORY_USAGE_MB)
# Application will log warnings and trigger cleanup at this level
MEMORY_WARNING_THRESHOLD_PERCENT=80

# Memory cleanup trigger threshold (% of MAX_MEMORY_USAGE_MB)
# Application will aggressively clean up memory at this level
MEMORY_CLEANUP_THRESHOLD_PERCENT=90

# Memory monitoring interval in seconds
MEMORY_MONITOR_INTERVAL_SECONDS=60

# ================================
# STRIKE OFFSET CONFIGURATION
# ================================

# Default strike offsets - standard range around ATM
# Format: comma-separated list of integer offsets
# 0 = At-The-Money (ATM)
# Negative = In-The-Money for calls, Out-The-Money for puts
# Positive = Out-The-Money for calls, In-The-Money for puts
DEFAULT_STRIKE_OFFSETS=-2,-1,0,1,2

# Extended strike offsets - wider range for comprehensive analysis
# Use for detailed market analysis and broader option chain coverage
EXTENDED_STRIKE_OFFSETS=-5,-4,-3,-2,-1,0,1,2,3,4,5

# Active strike offsets - currently used by the system
# SWITCHING PROCEDURE:
# 1. To use default range: Set to DEFAULT_STRIKE_OFFSETS value
# 2. To use extended range: Set to EXTENDED_STRIKE_OFFSETS value  
# 3. To use custom range: Specify your own comma-separated list
# 4. Restart application after changing
ACTIVE_STRIKE_OFFSETS=-2,-1,0,1,2

# Enable dynamic strike offset adjustment based on volatility
# true: Automatically expand offsets during high volatility periods
# false: Use fixed offsets as specified above
DYNAMIC_STRIKE_OFFSETS=false

# Strike offset expansion threshold (VIX level)
# When VIX > this value, system will use extended offsets
STRIKE_EXPANSION_VIX_THRESHOLD=25

# ================================
# DATA SECURITY & RETENTION
# ================================

# Data retention policy - how long to keep data
# infinite: Never automatically delete (RECOMMENDED for trading data)
# 30d: Delete after 30 days
# 90d: Delete after 90 days
# 1y: Delete after 1 year
#
# RECOMMENDATION: Use 'infinite' for trading data due to:
# - Regulatory compliance requirements
# - Long-term strategy backtesting
# - Audit trail maintenance
# - Historical analysis value
DATA_RETENTION_POLICY=infinite

# Enable data archival to save storage space
# Archives old data to compressed format while maintaining retention
# ARCHIVAL vs RETENTION:
# - Archival: Moves data to compressed storage (data preserved)
# - Retention: Actually deletes data (data lost)
ENABLE_ARCHIVAL=true

# Archive data older than specified days
# Data older than this moves to compressed archival storage
ARCHIVAL_AFTER_DAYS=30

# Archive compression level (1-9)
# Use maximum compression for archived data to save space
ARCHIVE_COMPRESSION_ENABLED=true
ARCHIVE_COMPRESSION_LEVEL=9

# Archive storage location
# Path where archived data files are stored
ARCHIVE_STORAGE_PATH=data/archive

# Enable data encryption at rest (optional security enhancement)
# Adds ~5% CPU overhead but provides additional security
# true: Encrypt stored data files (recommended for sensitive data)
# false: Store data unencrypted (better performance)
ENABLE_DATA_ENCRYPTION_AT_REST=false

# Data encryption key (only used if encryption enabled)
# SECURITY: Change this to a unique value for your installation
DATA_ENCRYPTION_KEY=your_unique_encryption_key_here

# Enable data integrity checks
# Periodically verify stored data has not been corrupted
ENABLE_DATA_INTEGRITY_CHECKS=true

# Data integrity check interval in hours
DATA_INTEGRITY_CHECK_INTERVAL_HOURS=24

# ================================
# INFLUXDB CONFIGURATION
# ================================

# InfluxDB connection details - for time-series data storage
# PROVIDED CREDENTIALS - your specific configuration
INFLUXDB_TOKEN=VFEhioeCi2vFCtv-dH_7Fe6gEgNtO-Tu7qcQW4WvIbAFQIdKGa_hDu4dxatOgwskZcva4CHkeOPbjkQwAvPyVg==
INFLUXDB_ORG=your-org
INFLUXDB_BUCKET=your-bucket
INFLUXDB_URL=http://localhost:8086

# InfluxDB retention policy - INFINITE RETENTION FOR AUDIT COMPLIANCE
# Set to 'infinite' to never automatically delete data
# This ensures complete audit trail and historical data availability
# CRITICAL: DO NOT change this to a time-limited value in production
INFLUXDB_RETENTION_POLICY=infinite

# InfluxDB write settings
INFLUXDB_WRITE_BATCH_SIZE=1000
INFLUXDB_WRITE_FLUSH_INTERVAL_MS=1000

# InfluxDB connection pool settings
INFLUXDB_CONNECTION_POOL_SIZE=10
INFLUXDB_CONNECTION_TIMEOUT_SECONDS=30

# Enable InfluxDB SSL/TLS (for production)
INFLUXDB_SSL_ENABLED=false
INFLUXDB_SSL_VERIFY=true

# InfluxDB precision for timestamps
# ns: Nanosecond (highest precision)
# us: Microsecond
# ms: Millisecond (recommended)
# s: Second
INFLUXDB_PRECISION=ms

# ================================
# REDIS CONFIGURATION
# ================================

# Redis connection settings - for caching and coordination
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# Redis connection pool settings
REDIS_CONNECTION_POOL_SIZE=20
REDIS_MAX_CONNECTIONS=50
REDIS_CONNECTION_TIMEOUT_SECONDS=10

# Redis authentication (if required)
REDIS_PASSWORD=
REDIS_USERNAME=

# Enable Redis SSL/TLS (for production)
REDIS_SSL_ENABLED=false
REDIS_SSL_CERT_REQS=none

# Redis caching settings
REDIS_DEFAULT_TTL_SECONDS=3600
REDIS_CACHE_KEY_PREFIX=optrading

# Redis cluster settings (if using Redis cluster)
REDIS_CLUSTER_ENABLED=false
REDIS_CLUSTER_NODES=localhost:6379

# ================================
# API CONFIGURATION
# ================================

# API server settings
API_HOST=0.0.0.0
API_PORT=8000

# API worker configuration based on deployment mode
# development: 1 worker (easier debugging)
# production: 4+ workers (better performance)
# WORKER SIZING GUIDELINES:
# CPU cores * 2 = good starting point
# Monitor CPU usage and adjust accordingly
API_WORKERS=4

# Enable hot reload in development
# SECURITY: Must be false in production
API_RELOAD=false

# API timeout settings
API_REQUEST_TIMEOUT_SECONDS=30
API_KEEP_ALIVE_TIMEOUT_SECONDS=5

# Enable API rate limiting
API_RATE_LIMITING_ENABLED=true
API_RATE_LIMIT_PER_MINUTE=100

# API CORS settings
API_CORS_ENABLED=true
API_CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# ================================
# PERFORMANCE CONFIGURATION
# ================================

# Processing batch size - number of items processed together
# SIZING BY DEPLOYMENT MODE:
# development: 100-500 (easier debugging)
# production: 1000+ (better throughput)
# 
# FACTORS TO CONSIDER:
# - Available memory (larger batches use more memory)
# - Processing complexity (complex operations need smaller batches)
# - Real-time requirements (smaller batches = lower latency)
PROCESSING_BATCH_SIZE=1000

# Maximum worker processes for parallel processing
# SIZING GUIDELINES:
# Small systems (4 cores): 2-4 workers
# Medium systems (8 cores): 4-8 workers
# Large systems (16+ cores): 8-16 workers
#
# CAUTION: Too many workers can cause memory pressure
PROCESSING_MAX_WORKERS=8

# Processing queue size - maximum pending items
PROCESSING_QUEUE_MAX_SIZE=10000

# Processing timeout per batch
PROCESSING_TIMEOUT_SECONDS=300

# Enable processing statistics collection
ENABLE_PROCESSING_STATS=true

# ================================
# SECURITY CONFIGURATION
# ================================

# Enable security features (recommended for production)
SECURITY_ENABLED=true

# API secret key for JWT token signing
# CRITICAL: Change this to a unique, strong key for production
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(64))"
API_SECRET_KEY=your_super_secret_key_here_change_this_in_production

# JWT token expiration time
JWT_EXPIRATION_HOURS=24
JWT_ALGORITHM=HS256

# Enable API key authentication
ENABLE_API_KEYS=true

# API key expiration (optional)
API_KEY_EXPIRATION_DAYS=365

# Enable request signing for additional security
ENABLE_REQUEST_SIGNING=false

# CORS security settings
CORS_ALLOW_CREDENTIALS=true
CORS_MAX_AGE_SECONDS=3600

# ================================
# TESTING CONFIGURATION
# ================================

# Enable integration tests
ENABLE_INTEGRATION_TESTS=true

# Enable performance testing
# true: Include performance benchmarks (recommended for production)
# false: Skip performance tests (faster development cycles)
ENABLE_PERFORMANCE_TESTS=true

# Test data cleanup after tests
# true: Clean up test data automatically
# false: Keep test data for inspection
TEST_DATA_CLEANUP=true

# Test timeout settings
TEST_TIMEOUT_SECONDS=300
INTEGRATION_TEST_TIMEOUT_SECONDS=600

# Test data generation settings
TEST_DATA_VOLUME_MULTIPLIER=1.0
TEST_STRESS_LEVEL=1

# ================================
# MONITORING & HEALTH CHECKS
# ================================

# Enable health check endpoints
ENABLE_HEALTH_CHECKS=true

# Health check interval based on deployment mode
# development: 30-60 seconds (more monitoring)
# production: 15-30 seconds (faster detection)
HEALTH_CHECK_INTERVAL_SECONDS=15

# Health check timeout
HEALTH_CHECK_TIMEOUT_SECONDS=10

# Enable auto-restart on service failures
# RECOMMENDATION: Enable in production for better uptime
AUTO_RESTART_ENABLED=true

# Auto-restart conditions
MAX_MEMORY_RESTART_MB=4096
MAX_ERROR_RATE_RESTART=50
RESTART_COOLDOWN_SECONDS=300

# Monitoring metrics collection
ENABLE_METRICS_COLLECTION=true
METRICS_COLLECTION_INTERVAL_SECONDS=30

# External monitoring integration
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=8080

# Grafana integration
GRAFANA_INTEGRATION_ENABLED=true

# ================================
# FEATURE FLAGS
# ================================

# Core feature enables
FEATURE_REAL_TIME_COLLECTION=true
FEATURE_HISTORICAL_ANALYSIS=true

# Analysis features
ENABLE_VIX_CORRELATION=true
ENABLE_SECTOR_BREADTH=true
ENABLE_GREEK_CALCULATIONS=true

# NEW ENHANCED FEATURES - Complete Integration
ENABLE_FII_ANALYSIS=true
ENABLE_DII_ANALYSIS=true
ENABLE_PRO_TRADER_ANALYSIS=true
ENABLE_CLIENT_ANALYSIS=true

# Price toggle functionality - NEW FEATURE
ENABLE_PRICE_TOGGLE=true
ENABLE_AVERAGE_PRICE_CALCULATION=true
DEFAULT_PRICE_MODE=LAST_PRICE

# Error detection panels - NEW FEATURE
ENABLE_ERROR_DETECTION_PANELS=true
ENABLE_AUTOMATED_ERROR_RECOVERY=true
ERROR_DETECTION_SENSITIVITY=NORMAL

# Advanced analytics features
ENABLE_OPTION_FLOW_ANALYSIS=true
ENABLE_UNUSUAL_ACTIVITY_DETECTION=true
ENABLE_SENTIMENT_ANALYSIS=true

# Dashboard features
ENABLE_INTERACTIVE_CHARTS=true
ENABLE_REAL_TIME_ALERTS=true
ENABLE_CUSTOM_NOTIFICATIONS=true

# ================================
# EXTERNAL API CONFIGURATION
# ================================

# External API timeout settings
EXTERNAL_API_TIMEOUT_SECONDS=30
EXTERNAL_API_RETRY_ATTEMPTS=3
EXTERNAL_API_RETRY_DELAY_SECONDS=5

# Rate limiting for external APIs
EXTERNAL_API_RATE_LIMIT_ENABLED=true
EXTERNAL_API_CALLS_PER_MINUTE=100

# External API caching
EXTERNAL_API_CACHE_ENABLED=true
EXTERNAL_API_CACHE_TTL_SECONDS=300

# ================================
# BACKUP CONFIGURATION
# ================================

# Enable automated backups
ENABLE_AUTOMATED_BACKUP=true

# Backup schedule settings
BACKUP_INTERVAL_HOURS=24
BACKUP_TIME_OF_DAY=02:00

# Backup retention settings
BACKUP_RETENTION_DAYS=30
BACKUP_COMPRESSION_ENABLED=true

# Backup storage location
BACKUP_STORAGE_PATH=backups/
BACKUP_REMOTE_ENABLED=false

# Backup verification
BACKUP_VERIFICATION_ENABLED=true
BACKUP_INTEGRITY_CHECK_ENABLED=true

# ================================
# NOTIFICATION CONFIGURATION
# ================================

# Email notification settings
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USE_TLS=true
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_password_here

# Notification recipients
ALERT_RECIPIENTS=admin@company.com,trading@company.com
CRITICAL_ALERT_RECIPIENTS=admin@company.com

# Notification thresholds
ERROR_RATE_ALERT_THRESHOLD=10
MEMORY_USAGE_ALERT_THRESHOLD_MB=3000
CPU_USAGE_ALERT_THRESHOLD_PERCENT=90

# Notification frequency limits
MIN_NOTIFICATION_INTERVAL_MINUTES=15
MAX_NOTIFICATIONS_PER_HOUR=10

# ================================
# ADVANCED CONFIGURATION
# ================================

# Timezone settings
TIMEZONE=Asia/Kolkata
MARKET_TIMEZONE=Asia/Kolkata

# Market hours configuration
MARKET_OPEN_TIME=09:15
MARKET_CLOSE_TIME=15:30
MARKET_PREOPEN_TIME=09:00

# Trading session settings
ENABLE_PREMARKET_ANALYSIS=true
ENABLE_AFTERMARKET_ANALYSIS=true

# Advanced processing settings
ENABLE_PARALLEL_PROCESSING=true
ENABLE_GPU_ACCELERATION=false
ENABLE_DISTRIBUTED_PROCESSING=false

# Memory optimization
ENABLE_MEMORY_POOLING=true
MEMORY_POOL_SIZE_MB=512
ENABLE_GARBAGE_COLLECTION_TUNING=true

# ================================
# KUBERNETES CONFIGURATION
# ================================

# Kubernetes deployment settings (if using K8s)
K8S_NAMESPACE=op-trading
K8S_SERVICE_ACCOUNT=op-trading-sa

# Auto-scaling settings
K8S_MIN_REPLICAS=2
K8S_MAX_REPLICAS=10
K8S_TARGET_CPU_UTILIZATION=70

# Resource limits
K8S_MEMORY_REQUEST=1Gi
K8S_MEMORY_LIMIT=2Gi
K8S_CPU_REQUEST=500m
K8S_CPU_LIMIT=1000m

# Health check settings for K8s
K8S_LIVENESS_PROBE_PATH=/health
K8S_READINESS_PROBE_PATH=/ready

# ================================
# DEVELOPMENT CONFIGURATION
# ================================

# Development-specific settings (only used when ENV=development)
DEV_ENABLE_PROFILING=false
DEV_ENABLE_DEBUG_TOOLBAR=false
DEV_HOT_RELOAD_ENABLED=true

# Development data settings
DEV_USE_SAMPLE_DATA=false
DEV_DATA_GENERATION_ENABLED=true

# Development testing
DEV_ENABLE_MOCK_SERVICES=true
DEV_BYPASS_AUTHENTICATION=false

# ================================
# MANUAL CONFIGURATION SECTION
# ================================
# The following settings require manual configuration with your actual values
# CRITICAL: These must be updated before production deployment

# Kite Connect API Credentials (REQUIRED for live data)
# Obtain from: https://kite.trade/connect/
# 1. Create/login to Kite Connect account
# 2. Create a new app
# 3. Copy API Key and API Secret
# 4. Update values below
KITE_API_KEY=your_api_key_here
KITE_API_SECRET=your_api_secret_here
KITE_ACCESS_TOKEN=your_access_token_here

# KITE CONNECT SETUP PROCEDURE:
# 1. Go to https://kite.trade/connect/
# 2. Login with your Zerodha account
# 3. Click "Create new app"
# 4. Fill app details:
#    - App name: OP Trading Platform
#    - App type: Connect
#    - Redirect URL: http://127.0.0.1:5000/
# 5. Copy API Key and Secret to above fields
# 6. Run authentication: python services/collection/kite_auth_manager.py --login
# 7. Copy generated access token to KITE_ACCESS_TOKEN field

# Email Settings (for alerts and notifications)
# GMAIL SETUP PROCEDURE:
# 1. Enable 2-factor authentication on your Gmail account
# 2. Go to Google Account settings > Security
# 3. Generate an "App Password" for this application
# 4. Use your Gmail address and app password below
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password_here

# Email recipients for different alert types
ALERT_RECIPIENTS=admin@company.com
CRITICAL_ALERT_RECIPIENTS=admin@company.com,cto@company.com
TRADING_ALERT_RECIPIENTS=trader@company.com

# Grafana Settings
# GRAFANA SETUP PROCEDURE:
# 1. Install Grafana: docker run -d -p 3000:3000 grafana/grafana
# 2. Login with admin/admin
# 3. Go to Configuration > API Keys
# 4. Create new API key with "Editor" role
# 5. Copy the generated key below
GRAFANA_URL=http://localhost:3000
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin
GRAFANA_API_KEY=your_grafana_api_key_here

# GRAFANA API KEY vs DATASOURCE UID EXPLANATION:
# - API Key: Used to authenticate with Grafana's REST API (for creating dashboards)
# - Datasource UID: Internal identifier for data sources within Grafana
# To get Grafana API Key: Configuration > API Keys > New API Key
# To get Datasource UID: Configuration > Data Sources > Click on your datasource

# Slack Integration (optional)
SLACK_WEBHOOK_URL=your_slack_webhook_url_here
SLACK_CHANNEL=#trading-alerts
SLACK_USERNAME=OP Trading Bot

# Database Backup Settings
BACKUP_FTP_HOST=your_backup_server.com
BACKUP_FTP_USER=backup_user
BACKUP_FTP_PASSWORD=backup_password

# External Data Sources (optional)
NSE_API_KEY=your_nse_api_key_here
BSE_API_KEY=your_bse_api_key_here

# ================================
# CONFIGURATION VALIDATION
# ================================

# Enable configuration validation on startup
VALIDATE_CONFIG_ON_STARTUP=true

# Fail on configuration errors
FAIL_ON_CONFIG_ERRORS=true

# Configuration file format validation
VALIDATE_CONFIG_FORMAT=true

# ================================
# DEBUGGING & TROUBLESHOOTING
# ================================

# Enable detailed error tracking
ENABLE_ERROR_TRACKING=true
ERROR_TRACKING_LEVEL=ERROR

# Enable request/response logging
ENABLE_REQUEST_LOGGING=false
ENABLE_RESPONSE_LOGGING=false

# Enable SQL query logging
ENABLE_SQL_LOGGING=false

# Performance profiling
ENABLE_PERFORMANCE_PROFILING=false
PROFILING_SAMPLE_RATE=0.01

# ================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ================================

# These settings can be overridden based on environment
# Format: SETTING_NAME_ENV=value
# Example: DEBUG_DEVELOPMENT=true

# Development overrides
DEBUG_DEVELOPMENT=true
LOG_LEVEL_DEVELOPMENT=DEBUG
MOCK_DATA_ENABLED_DEVELOPMENT=true

# Staging overrides
SECURITY_ENABLED_STAGING=true
ENABLE_PERFORMANCE_TESTS_STAGING=true

# Production overrides
DEBUG_PRODUCTION=false
LOG_LEVEL_PRODUCTION=INFO
SECURITY_ENABLED_PRODUCTION=true

# ================================
# CONFIGURATION CHANGE PROCEDURES
# ================================

# TO ENABLE/DISABLE SETTINGS:
# 1. Stop the application services
# 2. Edit this .env file
# 3. Change true/false values as needed
# 4. Restart the application services
# 5. Verify changes in application logs

# TO UPDATE CREDENTIALS:
# 1. Update the credential values in this file
# 2. Restart the application (no need to stop first)
# 3. Check logs for successful authentication

# TO CHANGE PERFORMANCE SETTINGS:
# 1. Monitor current resource usage
# 2. Update relevant settings gradually
# 3. Restart and monitor performance impact
# 4. Adjust further if needed

# TO MODIFY RETENTION POLICIES:
# WARNING: Changing retention policies can lead to data loss
# 1. Backup existing data first
# 2. Update retention settings
# 3. Restart application
# 4. Monitor disk usage changes

# ================================
# SUPPORT & TROUBLESHOOTING
# ================================

# For configuration help:
# 1. Check application logs: logs/application.log
# 2. Verify service status: docker ps or systemctl status
# 3. Test connectivity: ping localhost / telnet localhost 6379
# 4. Review error logs: logs/errors/

# Common configuration issues:
# 1. Database connection failures: Check INFLUXDB_* and REDIS_* settings
# 2. Authentication failures: Verify KITE_* credentials
# 3. Memory issues: Adjust MAX_MEMORY_USAGE_MB and buffer sizes
# 4. Performance problems: Tune batch sizes and worker counts

# Configuration validation command:
# python -c "from shared.config.settings import get_settings; settings = get_settings(); print('Configuration valid!')"

# ================================
# END OF CONFIGURATION FILE
# ================================

# Last updated: 2025-08-25 11:40 AM IST
# Configuration version: 1.0.0
# Total settings: 200+ comprehensive configuration options