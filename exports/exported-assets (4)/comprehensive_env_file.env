# OP Trading Platform - Complete Environment Configuration
# This file contains ALL configuration variables with detailed explanations
# Copy this to .env and update with your actual values

# ================================
# DEPLOYMENT MODE CONFIGURATION
# ================================

# Deployment Mode: Determines which services to start and how to configure them
# Options: "first_time", "development", "production"
# - first_time: Initial setup with guided configuration and testing
# - development: Live market testing with debugging features enabled
# - production: Production deployment with all monitoring and health checks
DEPLOYMENT_MODE=development

# Environment Type: Controls logging levels, security features, and performance optimizations
# Options: "development", "testing", "staging", "production"
# - development: Verbose logging, relaxed security, hot reload enabled
# - testing: Mock data sources, isolated testing environment
# - staging: Production-like environment for final testing
# - production: Maximum security, performance optimization, minimal logging
ENV=development

# Debug Mode: Enables detailed debugging information and error traces
# Set to true for development/troubleshooting, false for production
# When enabled: Full stack traces, detailed error messages, debug endpoints
# When disabled: Minimal error exposure, production-safe error handling
DEBUG=true

# Logging Level: Controls the verbosity of application logs
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# - DEBUG: All log messages including detailed debug information
# - INFO: General application flow and important events
# - WARNING: Potentially harmful situations
# - ERROR: Error events but application continues
# - CRITICAL: Serious errors that may abort the application
LOG_LEVEL=INFO

# Application Version: Used for deployment tracking and compatibility checks
VERSION=1.0.0

# ================================
# BROKER API CONFIGURATION (Zerodha Kite Connect)
# ================================

# Kite Connect API Credentials
# These are obtained from https://kite.trade/connect/
# IMPORTANT: Keep these credentials secure and never commit them to version control

# API Key: Your Kite Connect app's API key (public identifier)
# How to get: Login to Kite Connect portal → Create App → Copy API Key
# Example: abc123def456ghi789jkl012mno345pq
KITE_API_KEY=your_api_key_here

# API Secret: Your Kite Connect app's secret key (private key)
# How to get: Same as API Key but keep this absolutely secret
# Used for: Generating access tokens and authentication
# Example: xyz789abc123def456ghi789jkl012
KITE_API_SECRET=your_api_secret_here

# Access Token: Current session access token (changes daily)
# How to get: Generated through OAuth flow or manual login process
# Auto-managed: Our system can generate this automatically via kite_auth_manager.py
# Example: abc123xyz789def456ghi789jkl012mno345
KITE_ACCESS_TOKEN=your_access_token_here

# Request Token: Temporary token from Kite Connect login (used once)
# How to get: Obtained during OAuth login flow
# Usage: Only needed for initial token generation, not for regular operations
KITE_REQUEST_TOKEN=

# Kite API Configuration
# API Root: Base URL for Kite Connect API (should not be changed)
KITE_API_ROOT=https://api.kite.trade

# Login URL: Kite Connect OAuth login URL (should not be changed)
KITE_LOGIN_URL=https://kite.zerodha.com/connect/login

# Session Configuration
KITE_SESSION_EXPIRY_HOOK=
KITE_MICRO_CACHE=true
KITE_POOL_SIZE=10

# Rate Limiting: Controls API request frequency to comply with Kite limits
# Kite allows 3 requests per second, we set conservative limits
KITE_RATE_LIMIT=10
KITE_TIMEOUT=30

# Risk Management Settings
# Maximum orders per second: Prevents accidental order flooding
MAX_ORDERS_PER_SECOND=10

# Maximum position size: Risk control for position sizing (in rupees)
MAX_POSITION_SIZE=1000000

# Position timeout: How long to wait for position updates (seconds)
POSITION_TIMEOUT_SECONDS=300

# Order validation: Enable pre-flight order validation
ORDER_VALIDATION=true

# Paper trading: Set to true to simulate trading without real orders
PAPER_TRADING=false

# ================================
# DATA SOURCE CONFIGURATION
# ================================

# Data Source Mode: Determines whether to use live or mock data
# Options: "live", "mock", "hybrid"
# - live: Use real broker API data (requires valid credentials)
# - mock: Use simulated data for testing/development
# - hybrid: Use live data when available, fallback to mock
DATA_SOURCE_MODE=mock

# Live Data Settings (when DATA_SOURCE_MODE=live or hybrid)
ENABLE_LIVE_DATA_COLLECTION=true
LIVE_DATA_FALLBACK_TO_MOCK=true
LIVE_DATA_RETRY_ATTEMPTS=3
LIVE_DATA_RETRY_DELAY=5

# Mock Data Settings (when DATA_SOURCE_MODE=mock or hybrid)
MOCK_DATA_ENABLED=true
MOCK_DATA_REALISTIC_PRICING=true
MOCK_DATA_VOLATILITY_FACTOR=0.02
MOCK_DATA_UPDATE_INTERVAL=30

# ================================
# DATABASE CONFIGURATION
# ================================

# InfluxDB Settings (Time-Series Database for metrics and analytics)
# URL: InfluxDB server endpoint
INFLUXDB_URL=http://localhost:8086

# Organization: Your InfluxDB organization name
# How to get: Created during InfluxDB setup or from InfluxDB UI
INFLUXDB_ORG=your-org

# Bucket: InfluxDB bucket for storing options data
# How to get: Created during InfluxDB setup or from InfluxDB UI
INFLUXDB_BUCKET=your-bucket

# Token: InfluxDB authentication token
# How to get: InfluxDB UI → Data → Tokens → Generate Token
# Your provided token:
INFLUXDB_TOKEN=VFEhioeCi2vFCtv-dH_7Fe6gEgNtO-Tu7qcQW4WvIbAFQIdKGa_hDu4dxatOgwskZcva4CHkeOPbjkQwAvPyVg==

# Database Credentials (legacy settings for older InfluxDB versions)
INFLUXDB_USERNAME=admin
INFLUXDB_PASSWORD=your_influxdb_password

# Data Retention Policy
# INFINITE: Data never expires (recommended for historical analysis)
# Format: Xd (days), Xw (weeks), Xm (months), Xy (years)
# WARNING: Infinite retention requires adequate disk space planning
INFLUXDB_RETENTION_POLICY=infinite

# Write Precision: Timestamp precision for data writes
# Options: ns (nanoseconds), us (microseconds), ms (milliseconds), s (seconds)
# ms is recommended for financial data
INFLUXDB_PRECISION=ms

# Batch Processing Settings
# Batch size: Number of data points to write in one batch (higher = more efficient)
# Optimal range: 1000-5000 depending on data size
INFLUXDB_BATCH_SIZE=1000

# Flush interval: How often to force write batches (seconds)
# Lower values = more real-time data, higher CPU usage
INFLUXDB_FLUSH_INTERVAL=10

# Connection Security Settings
INFLUXDB_SSL=false
INFLUXDB_VERIFY_SSL=true
INFLUXDB_TIMEOUT=30

# ================================
# REDIS CONFIGURATION (Message Queue & Caching)
# ================================

# Redis Connection Settings
# Host: Redis server hostname or IP
REDIS_HOST=localhost

# Port: Redis server port (default: 6379)
REDIS_PORT=6379

# Database: Redis database number (0-15, use different numbers for isolation)
REDIS_DB=0

# Authentication: Redis password (leave empty if no auth)
REDIS_PASSWORD=

# Username: Redis username (for Redis 6.0+ ACL)
REDIS_USERNAME=

# Performance Settings
# Max connections: Maximum number of Redis connections in pool
# Higher values allow more concurrent operations but use more memory
REDIS_MAX_CONNECTIONS=100

# Timeouts: Connection timeout settings (seconds)
# Socket timeout: How long to wait for responses
REDIS_SOCKET_TIMEOUT=30.0

# Connect timeout: How long to wait for initial connection
REDIS_SOCKET_CONNECT_TIMEOUT=30.0

# Health check: How often to verify Redis connectivity (seconds)
REDIS_HEALTH_CHECK_INTERVAL=30

# Retry settings: Behavior when Redis operations fail
REDIS_RETRY_ON_TIMEOUT=true
REDIS_DECODE_RESPONSES=true

# Redis Cluster Settings (for high availability deployments)
# Enable cluster: Set to true if using Redis Cluster
REDIS_CLUSTER_ENABLED=false

# Cluster nodes: Comma-separated list of cluster node endpoints
REDIS_CLUSTER_NODES=localhost:6379,localhost:6380,localhost:6381

# Coverage requirement: Whether all slots must be covered
REDIS_CLUSTER_REQUIRE_FULL_COVERAGE=false

# ================================
# FILE STORAGE CONFIGURATION
# ================================

# Storage Paths: Define where different types of data are stored
# Base directory: Root directory for all data storage
BASE_DATA_DIR=data

# CSV Data: Structured option leg data in CSV format
CSV_DATA_ROOT=data/csv_data

# JSON Snapshots: Raw JSON responses for audit trail
JSON_SNAPSHOTS_ROOT=data/json_snapshots

# Analytics Output: Computed analytics results
ANALYTICS_OUTPUT_ROOT=data/analytics

# Logs: Application log files
LOGS_ROOT=logs

# Backups: Data backups for disaster recovery
BACKUP_ROOT=backups

# ================================
# MEMORY MAPPING CONFIGURATION
# ================================

# Memory Mapping: Use OS-level memory mapping for large files
# EXPLANATION: Memory mapping allows the OS to manage file I/O more efficiently
# - When enabled: Files are mapped directly into memory, faster access
# - When disabled: Traditional file I/O operations, more memory overhead
# - Recommended: true for large datasets, false for small datasets or limited RAM
USE_MEMORY_MAPPING=true

# Memory Mapping Settings
# Max mapped file size: Maximum size of files to memory map (MB)
# Files larger than this will use traditional I/O
MAX_MEMORY_MAPPED_FILE_SIZE_MB=100

# Memory mapping mode: How aggressively to use memory mapping
# Options: "conservative", "aggressive", "adaptive"
# - conservative: Only map frequently accessed files
# - aggressive: Map all eligible files
# - adaptive: Dynamically adjust based on system resources
MEMORY_MAPPING_MODE=adaptive

# ================================
# COMPRESSION CONFIGURATION
# ================================

# Compression: Reduce storage space by compressing data files
# EXPLANATION: Compression trades CPU time for storage space
# - When enabled: Files are compressed, uses less disk space, more CPU
# - When disabled: Files stored uncompressed, uses more disk, less CPU
# - Recommendation: Enable for production with adequate CPU, disable for development
COMPRESSION_ENABLED=false

# Compression Level: How aggressively to compress (1-9)
# EXPLANATION: Higher levels mean smaller files but more CPU usage
# - Level 1: Fastest compression, larger files
# - Level 6: Balanced compression (recommended)
# - Level 9: Maximum compression, slowest
# - Trade-off: Level 6 provides good balance of speed and compression
COMPRESSION_LEVEL=6

# Compression Algorithm: Which compression algorithm to use
# Options: "gzip", "lz4", "zstd"
# - gzip: Standard compression, good compatibility
# - lz4: Fast compression/decompression, moderate compression ratio
# - zstd: Modern algorithm, excellent compression ratio and speed
COMPRESSION_ALGORITHM=gzip

# ================================
# BUFFER SIZE CONFIGURATION
# ================================

# CSV Buffer Size: Memory buffer for CSV file operations (bytes)
# EXPLANATION: Larger buffers reduce I/O operations but use more memory
# - Small buffer (1024): Less memory usage, more I/O operations
# - Large buffer (65536): More memory usage, fewer I/O operations
# - Recommended: 8192 for balanced performance
# - Calculation: Each CSV row ~200 bytes, 8192 buffer = ~40 rows per operation
CSV_BUFFER_SIZE=8192

# JSON Buffer Size: Memory buffer for JSON file operations (bytes)
# EXPLANATION: JSON files are typically larger than CSV rows
# - JSON snapshots can be 5-50KB each
# - 16384 buffer allows efficient processing of most JSON responses
# - Increase for very large option chains, decrease for memory-constrained systems
JSON_BUFFER_SIZE=16384

# Network Buffer Size: Buffer for network I/O operations (bytes)
# Used for HTTP requests/responses with broker APIs
NETWORK_BUFFER_SIZE=32768

# ================================
# MEMORY USAGE LIMITS
# ================================

# Maximum Memory Usage: Soft limit for total application memory (MB)
# EXPLANATION: This is a guideline, not a hard limit
# - Purpose: Prevents system overload and maintains responsiveness
# - Behavior: Application will try to stay under this limit
# - When exceeded: Triggers garbage collection and memory optimization
# - Setting: 50-70% of available system RAM
# - Your setting: 2048 MB (2GB) - suitable for 4-8GB systems
MAX_MEMORY_USAGE_MB=2048

# Memory Monitoring: How often to check memory usage (seconds)
# Lower values = more responsive but higher overhead
MEMORY_MONITORING_INTERVAL=60

# Memory Warning Threshold: Percentage of max memory to trigger warnings
# Example: 80% means warnings when using >1638 MB (80% of 2048 MB)
MEMORY_WARNING_THRESHOLD=80

# Memory Critical Threshold: Percentage to trigger aggressive cleanup
# Example: 90% means cleanup when using >1843 MB (90% of 2048 MB)
MEMORY_CRITICAL_THRESHOLD=90

# Garbage Collection: Force periodic memory cleanup
ENABLE_PERIODIC_GC=true
GC_INTERVAL_SECONDS=300

# ================================
# DATA PROCESSING CONFIGURATION
# ================================

# Incremental Processing: Only process new/changed data
# EXPLANATION: Dramatically improves performance for large datasets
# - When enabled: Tracks what's already processed, skips duplicate work
# - When disabled: Reprocesses all data every time (slower but simpler)
# - Mechanism: Uses file cursors and checksums to track progress
# - Recommended: true for production, false only for debugging
ENABLE_INCREMENTAL=true

# Processing Batch Size: Number of records to process at once
# EXPLANATION: Balance between memory usage and processing efficiency
# - Small batches (100): Less memory, more processing overhead
# - Large batches (10000): More memory, less processing overhead
# - Optimal: 1000-2000 for most systems
# - Calculation: Each option leg ~1KB, 1000 batch = ~1MB memory
PROCESSING_BATCH_SIZE=1000

# Maximum Workers: Number of parallel processing threads
# EXPLANATION: More workers = faster processing but more resource usage
# - Setting: Usually CPU cores - 1 (leave one core for system)
# - Example: 4-core system → 3 workers, 8-core system → 7 workers
# - Your setting: 8 workers (good for 8+ core systems)
PROCESSING_MAX_WORKERS=8

# Worker timeout: How long to wait for worker completion (seconds)
WORKER_TIMEOUT_SECONDS=300

# Enable multiprocessing: Use multiple processes instead of threads
# EXPLANATION: Processes avoid Python GIL but use more memory
# - threads: Lower memory overhead, limited by Python GIL
# - processes: Higher memory overhead, true parallelism
# - Recommended: true for CPU-intensive tasks, false for I/O-intensive
ENABLE_MULTIPROCESSING=true

# ================================
# ARCHIVAL AND RETENTION
# ================================

# Data Archival: Move old data to compressed storage
# EXPLANATION: Keeps recent data fast while preserving historical data
# - Recent data: Fast access for real-time operations
# - Archived data: Compressed storage for historical analysis
# - Process: Data older than threshold moved to archive format
ENABLE_ARCHIVAL=true

# Archive threshold: Age of data before archival (days)
# Example: 30 means data older than 30 days gets archived
ARCHIVAL_AFTER_DAYS=30

# Archive compression: Compress archived data
# EXPLANATION: Archived data accessed infrequently, high compression beneficial
ARCHIVE_COMPRESSION_ENABLED=true
ARCHIVE_COMPRESSION_LEVEL=9

# Data Retention: How long to keep data before deletion
# EXPLANATION: Eventually delete very old data to manage storage
# - infinite: Never delete (requires unlimited storage growth planning)
# - Xd: Delete after X days
# - Recommendation: "infinite" for trading data (historical value)
DATA_RETENTION_PERIOD=infinite

# ================================
# STRIKE OFFSET CONFIGURATION
# ================================

# Strike Offsets: Which option strikes to collect relative to ATM
# EXPLANATION: ATM (At The Money) is the strike nearest to spot price
# - Offset 0: ATM strike
# - Offset +1: One strike above ATM (OTM calls, ITM puts)
# - Offset -1: One strike below ATM (ITM calls, OTM puts)

# Default Strike Offsets: Standard set for regular operations
# [-2, -1, 0, 1, 2] covers 5 strikes around ATM
DEFAULT_STRIKE_OFFSETS=-2,-1,0,1,2

# Extended Strike Offsets: Wider range for comprehensive analysis
# [-5 to +5] covers 11 strikes for deeper analysis
EXTENDED_STRIKE_OFFSETS=-5,-4,-3,-2,-1,0,1,2,3,4,5

# Active Strike Offsets: Currently used set (can be switched)
# To switch: Change this to DEFAULT_STRIKE_OFFSETS or EXTENDED_STRIKE_OFFSETS
# or create custom set like: -3,-2,-1,0,1,2,3
ACTIVE_STRIKE_OFFSETS=-2,-1,0,1,2

# How to switch between default and extended:
# 1. For regular operations: Set ACTIVE_STRIKE_OFFSETS=${DEFAULT_STRIKE_OFFSETS}
# 2. For comprehensive analysis: Set ACTIVE_STRIKE_OFFSETS=${EXTENDED_STRIKE_OFFSETS}
# 3. For custom range: Set specific offsets like: ACTIVE_STRIKE_OFFSETS=-1,0,1

# ================================
# API CONFIGURATION
# ================================

# FastAPI Server Settings
# Host: IP address to bind server (0.0.0.0 = all interfaces)
API_HOST=0.0.0.0

# Port: TCP port for API server
API_PORT=8000

# Workers: Number of API server worker processes
# Recommendation: 2 * CPU cores for I/O bound applications
API_WORKERS=4

# Auto-reload: Restart server when code changes (development only)
API_RELOAD=false

# Access logging: Log all API requests
API_ACCESS_LOG=true

# Proxy headers: Trust X-Forwarded-* headers (needed for load balancers)
API_PROXY_HEADERS=true

# ================================
# SECURITY CONFIGURATION
# ================================

# General Security Settings
SECURITY_ENABLED=true

# Secret Key: Used for JWT token signing and encryption
# IMPORTANT: Generate a random 32+ character string for production
# Example generation: openssl rand -hex 32
# Keep this secret and never share it
API_SECRET_KEY=your_super_secret_key_here_change_this_in_production

# JWT Configuration
# Algorithm: Cryptographic algorithm for JWT signing
JWT_ALGORITHM=HS256

# Token expiration: How long JWT tokens remain valid
JWT_EXPIRATION_HOURS=24

# Refresh token: Longer-lived tokens for renewing access
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# API Key Authentication
# Enable API keys: Allow authentication using API keys
ENABLE_API_KEYS=true

# API key length: Length of generated API keys (characters)
API_KEY_LENGTH=32

# CORS Configuration
# CORS: Cross-Origin Resource Sharing (allows web browsers to access API)
CORS_ENABLED=true

# Origins: Which domains can access the API (* = all domains)
# Production: Specify exact domains like: https://yourdomain.com,https://app.yourdomain.com
# Development: * for convenience
CORS_ORIGINS=*

# Methods: Which HTTP methods are allowed
CORS_METHODS=GET,POST,PUT,DELETE,OPTIONS

# Headers: Which headers browsers can send
CORS_HEADERS=*

# Rate Limiting
# API rate limiting: Prevent abuse by limiting requests per time period
API_RATE_LIMIT_ENABLED=true

# Calls per period: Number of requests allowed
API_RATE_LIMIT_CALLS=100

# Period: Time window for rate limiting (seconds)
API_RATE_LIMIT_PERIOD=60

# Per-IP limiting: Apply rate limits per IP address
API_RATE_LIMIT_PER_IP=true

# ================================
# DATA SECURITY CONFIGURATION
# ================================

# Data Encryption: Encrypt sensitive data at rest
# EXPLANATION: Adds security layer for stored data
# - Performance impact: ~5-10% CPU overhead
# - Security benefit: Data unreadable if files stolen
ENABLE_DATA_ENCRYPTION_AT_REST=false

# Encryption key: Key for data encryption (if enabled)
# Generate: openssl rand -base64 32
DATA_ENCRYPTION_KEY=your_32_char_encryption_key_here

# Hashing algorithm: For password hashing and data integrity
HASH_ALGORITHM=sha256

# Salt rounds: For password hashing (higher = more secure, slower)
SALT_ROUNDS=12

# Network Security
# Allowed hosts: Which hostnames can access the application
ALLOWED_HOSTS=localhost,127.0.0.1,api.your-domain.com

# Trusted proxies: IP ranges of trusted proxy servers
TRUSTED_PROXIES=127.0.0.1,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16

# HTTPS Configuration
# Enable HTTPS: Use encrypted connections (required for production)
ENABLE_HTTPS=false

# SSL certificate paths (if using HTTPS)
SSL_CERT_PATH=
SSL_KEY_PATH=

# Data Retention and Audit
# Secure deletion: Overwrite deleted files
SECURE_DELETE_ENABLED=true

# Audit logging: Log all data access and modifications
AUDIT_LOG_ENABLED=true

# ================================
# STRUCTURED LOGGING CONFIGURATION
# ================================

# Structured Logging: Use JSON format for logs with metadata
# EXPLANATION: Makes logs machine-readable and searchable
# - Benefits: Easy parsing, correlation, filtering
# - Format: JSON with standardized fields
ENABLE_STRUCTURED_LOGGING=true

# Log format: How to format log messages
# Options: "json", "text", "combined"
LOG_FORMAT=json

# Trace ID: Unique identifier for request chains across services
# EXPLANATION: Helps trace a request through multiple services
# - Example: Request → API → Collection → Processing → Analytics
# - Same trace ID appears in all service logs for this request chain
INCLUDE_TRACE_ID=true

# Request ID: Unique identifier for individual API requests
# EXPLANATION: Helps correlate all log entries for a specific request
# - Generated for each incoming API request
# - Included in all log entries while processing that request
INCLUDE_REQUEST_ID=true

# Additional context fields
INCLUDE_USER_ID=true
INCLUDE_SESSION_ID=true
INCLUDE_CORRELATION_ID=true

# Log metadata: Additional information to include
LOG_INCLUDE_HOSTNAME=true
LOG_INCLUDE_PROCESS_ID=true
LOG_INCLUDE_THREAD_ID=true

# ================================
# MONITORING & HEALTH CONFIGURATION
# ================================

# Health Check Intervals
# Health check: How often to check service health (seconds)
HEALTH_CHECK_INTERVAL=30

# System monitoring: How often to collect system metrics (seconds)
SYSTEM_METRICS_INTERVAL=60

# Application monitoring: How often to check app metrics (seconds)
APPLICATION_METRICS_INTERVAL=30

# Health Check Settings
ENABLE_HEALTH_CHECKS=true
HEALTH_CHECK_TIMEOUT=10

# Self-Healing: Automatically restart failed services
AUTO_RESTART_ENABLED=true

# Restart attempts: How many times to try restarting a failed service
MAX_RESTART_ATTEMPTS=3

# Restart delay: How long to wait between restart attempts (seconds)
RESTART_DELAY_SECONDS=60

# Alert Thresholds
# CPU usage: Alert when CPU usage exceeds threshold (percentage)
CPU_ALERT_THRESHOLD=80

# Memory usage: Alert when memory usage exceeds threshold (percentage)  
MEMORY_ALERT_THRESHOLD=85

# Disk usage: Alert when disk usage exceeds threshold (percentage)
DISK_ALERT_THRESHOLD=90

# Network usage: Alert when network usage exceeds threshold (percentage)
NETWORK_ALERT_THRESHOLD=80

# Application-specific thresholds
# Error rate: Alert when error rate exceeds threshold (0.0-1.0)
ERROR_RATE_THRESHOLD=0.05

# Response time: Alert when average response time exceeds threshold (milliseconds)
RESPONSE_TIME_THRESHOLD=5000

# Data freshness: Alert when data is older than threshold (seconds)
DATA_FRESHNESS_THRESHOLD=300

# Trading-specific thresholds
# Data freshness: Alert if option data is stale (minutes)
ALERT_DATA_FRESHNESS_MINUTES=5

# Processing delay: Alert if processing is delayed (minutes)
ALERT_PROCESSING_DELAY_MINUTES=2

# Error rate: Alert if error rate is high (0.0-1.0)
ALERT_ERROR_RATE_THRESHOLD=0.10

# Memory usage: Alert if memory usage is high (0.0-1.0)
ALERT_MEMORY_USAGE_THRESHOLD=0.85

# Disk usage: Alert if disk usage is high (0.0-1.0)
ALERT_DISK_USAGE_THRESHOLD=0.90

# ================================
# EMAIL NOTIFICATION CONFIGURATION
# ================================

# SMTP Server Settings for sending alerts via email
# Server: SMTP server hostname (Gmail: smtp.gmail.com)
SMTP_SERVER=smtp.gmail.com

# Port: SMTP server port
# 587: TLS/STARTTLS (recommended)
# 465: SSL/TLS
# 25: Unencrypted (not recommended)
SMTP_PORT=587

# Security: Use encrypted connection
SMTP_USE_TLS=true
SMTP_USE_SSL=false

# Credentials: Email account for sending alerts
# For Gmail: Use app-specific password, not regular password
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password_here

# Connection timeout (seconds)
SMTP_TIMEOUT=30

# Email Recipients
# Alert recipients: Who gets critical alerts (comma-separated)
ALERT_RECIPIENTS=admin@company.com,ops@company.com

# Notification recipients: Who gets general notifications
NOTIFICATION_RECIPIENTS=team@company.com

# Error recipients: Who gets error reports
ERROR_RECIPIENTS=errors@company.com

# Email Configuration
# Templates directory: Where email templates are stored
EMAIL_TEMPLATES_DIR=templates/email

# HTML email: Send formatted HTML emails
ENABLE_HTML_EMAIL=true

# Subject prefix: Add prefix to all email subjects
EMAIL_SUBJECT_PREFIX=[OP-Trading]

# ================================
# MARKET DATA CONFIGURATION
# ================================

# Supported Indices: Which market indices to monitor
SUPPORTED_INDICES=NIFTY,BANKNIFTY,SENSEX,FINNIFTY,MIDCPNIFTY

# Default buckets: Which expiry buckets to collect
DEFAULT_BUCKETS=this_week,next_week,this_month,next_month

# Market Timing (All times in IST)
# Pre-open: Market preparation period
MARKET_PRE_OPEN_TIME=09:00

# Market open: Regular trading session start
MARKET_OPEN_TIME=09:15

# Market close: Regular trading session end
MARKET_CLOSE_TIME=15:30

# Post-close: After-hours period
MARKET_POST_CLOSE_TIME=16:00

# Data Quality Settings
# Enable quality checks: Validate data for consistency
DATA_QUALITY_ENABLED=true

# Minimum quality score: Reject data below this score (0-100)
MIN_DATA_QUALITY_SCORE=70

# Outlier detection: Identify and handle unusual data points
OUTLIER_DETECTION_ENABLED=true

# Price validation: Check prices for reasonableness
PRICE_VALIDATION_ENABLED=true

# Volume validation: Check volumes for reasonableness
VOLUME_VALIDATION_ENABLED=true

# ================================
# ANALYTICS CONFIGURATION
# ================================

# Options Analytics Features
# Greeks calculation: Compute delta, gamma, theta, vega
ENABLE_GREEKS_CALCULATION=true

# Implied volatility: Calculate IV using Newton-Raphson method
ENABLE_IV_CALCULATION=true

# Volatility surface: Generate 3D volatility surface
ENABLE_VOLATILITY_SURFACE=true

# Put-Call Ratio analysis: Analyze put/call ratios
ENABLE_PCR_ANALYSIS=true

# Max pain: Calculate maximum pain levels
ENABLE_MAX_PAIN_CALCULATION=true

# Market sentiment: Aggregate sentiment indicators
ENABLE_SENTIMENT_ANALYSIS=true

# Financial Model Parameters
# Risk-free rate: Used in Black-Scholes calculations (annual rate)
RISK_FREE_RATE=0.06

# Dividend yield: Expected dividend yield (annual rate)
DIVIDEND_YIELD=0.01

# Advanced Analytics Features
# VIX correlation: Analyze correlation with volatility index
ENABLE_VIX_CORRELATION=true

# Sector breadth: Market breadth analysis
ENABLE_SECTOR_BREADTH=true

# FII analysis: Foreign Institutional Investor activity
ENABLE_FII_ANALYSIS=true

# DII analysis: Domestic Institutional Investor activity
ENABLE_DII_ANALYSIS=true

# Options flow: Analyze options order flow
ENABLE_OPTION_FLOW_ANALYSIS=true

# Machine Learning (Advanced)
# Enable ML models: Use machine learning for predictions
ENABLE_ML_MODELS=false

# Model path: Where ML models are stored
ML_MODEL_PATH=models/

# Retrain interval: How often to retrain models (hours)
ML_RETRAIN_INTERVAL_HOURS=24

# Confidence threshold: Minimum confidence for predictions (0.0-1.0)
ML_PREDICTION_CONFIDENCE_THRESHOLD=0.8

# ================================
# PROMETHEUS METRICS CONFIGURATION
# ================================

# Prometheus Settings
# URL: Prometheus server endpoint
PROMETHEUS_URL=http://localhost:9090

# Job name: Identifier for this application in Prometheus
PROMETHEUS_JOB_NAME=op-trading

# Instance: Instance identifier for this deployment
PROMETHEUS_INSTANCE=localhost:8000

# Enable metrics: Collect and expose Prometheus metrics
ENABLE_PROMETHEUS_METRICS=true

# Metrics server settings
# Port: Port for metrics endpoint
METRICS_PORT=8080

# Path: URL path for metrics endpoint
METRICS_PATH=/metrics

# Metric collection settings
COLLECT_SYSTEM_METRICS=true
COLLECT_APPLICATION_METRICS=true
COLLECT_BUSINESS_METRICS=true

# ================================
# GRAFANA CONFIGURATION
# ================================

# Grafana Settings
# URL: Grafana server endpoint
GRAFANA_URL=http://localhost:3000

# Default admin credentials (change these!)
GRAFANA_USER=admin
GRAFANA_PASSWORD=admin

# Organization ID: Grafana organization (usually 1 for default)
GRAFANA_ORG_ID=1

# API Key: Grafana API key for programmatic access
# How to get: Grafana UI → Configuration → API Keys → Add API key
# Note: This is different from datasource UID
GRAFANA_API_KEY=your_grafana_api_key

# Dashboard settings
# Directory: Where dashboard JSON files are stored
GRAFANA_DASHBOARDS_DIR=infrastructure/grafana/dashboards

# Provisioning: Automatically load dashboards on startup
ENABLE_DASHBOARD_PROVISIONING=true

# Refresh: How often dashboards refresh data
DASHBOARD_REFRESH_INTERVAL=30s

# Time range: Default time range for dashboards
DASHBOARD_TIME_RANGE=6h

# ================================
# CONTAINER & KUBERNETES CONFIGURATION
# ================================

# Docker Settings
# Image tag: Version tag for Docker images
DOCKER_IMAGE_TAG=latest

# Build target: Which Dockerfile stage to use
DOCKER_BUILD_TARGET=production

# Network: Docker network name
DOCKER_NETWORK=op-network

# Project name: Docker Compose project identifier
DOCKER_COMPOSE_PROJECT_NAME=op-trading

# Kubernetes Settings
# Namespace: Kubernetes namespace for deployment
K8S_NAMESPACE=op-trading

# Deployment name: Name of Kubernetes deployment
K8S_DEPLOYMENT_NAME=op-trading

# Service account: Kubernetes service account
K8S_SERVICE_ACCOUNT=op-trading-sa

# Config map: Name of Kubernetes config map
K8S_CONFIG_MAP=op-config

# Secret: Name of Kubernetes secret
K8S_SECRET_NAME=op-secrets

# Load Balancer Settings
ENABLE_LOAD_BALANCER=false
LB_ALGORITHM=round_robin
LB_HEALTH_CHECK_INTERVAL=30
LB_MAX_RETRIES=3

# ================================
# BACKUP & RECOVERY CONFIGURATION
# ================================

# Automated Backup Settings
# Enable backups: Automatically backup data
ENABLE_AUTOMATED_BACKUP=true

# Backup interval: How often to create backups (hours)
BACKUP_INTERVAL_HOURS=24

# Retention: How long to keep backups (days)
BACKUP_RETENTION_DAYS=30

# Compression: Compress backup files
BACKUP_COMPRESSION=true

# Encryption: Encrypt backup files
BACKUP_ENCRYPTION=false

# Recovery Settings
# Point-in-time recovery: Enable recovery to specific timestamps
ENABLE_POINT_IN_TIME_RECOVERY=true

# Recovery log retention: How long to keep recovery logs (days)
RECOVERY_LOG_RETENTION_DAYS=7

# Auto-recovery: Automatically recover from data corruption
AUTO_RECOVERY_ENABLED=false

# Recovery verification: Verify recovered data integrity
RECOVERY_VERIFICATION_ENABLED=true

# Cloud Backup (Optional)
CLOUD_BACKUP_ENABLED=false
CLOUD_BACKUP_PROVIDER=aws
CLOUD_BACKUP_BUCKET=
CLOUD_BACKUP_REGION=
CLOUD_BACKUP_ACCESS_KEY=
CLOUD_BACKUP_SECRET_KEY=

# ================================
# TESTING CONFIGURATION
# ================================

# Test Environment Settings
# Test data directory: Where test data files are stored
TEST_DATA_DIR=tests/test_data

# Mock settings: Use mock services for testing
MOCK_BROKER_API=false
MOCK_DATABASE=false
MOCK_REDIS=false

# Test execution settings
ENABLE_INTEGRATION_TESTS=true
ENABLE_PERFORMANCE_TESTS=false
ENABLE_CHAOS_TESTING=false

# Test database settings (separate from production)
TEST_INFLUXDB_URL=http://localhost:8087
TEST_REDIS_HOST=localhost
TEST_REDIS_PORT=6380

# Cleanup: Clean up test data after tests
TEST_DATA_CLEANUP=true

# ================================
# FEATURE FLAGS
# ================================

# Core Features
FEATURE_REAL_TIME_COLLECTION=true
FEATURE_HISTORICAL_ANALYSIS=true
FEATURE_PREDICTIVE_ANALYTICS=false
FEATURE_AUTOMATED_TRADING=false
FEATURE_PORTFOLIO_MANAGEMENT=false

# Advanced Features
FEATURE_MACHINE_LEARNING=false
FEATURE_SENTIMENT_ANALYSIS=true
FEATURE_TECHNICAL_INDICATORS=true
FEATURE_BACKTESTING=false
FEATURE_PAPER_TRADING=false

# Experimental Features
FEATURE_EXPERIMENTAL_ANALYTICS=false
FEATURE_BETA_API_ENDPOINTS=false
FEATURE_ADVANCED_VISUALIZATIONS=true
FEATURE_MOBILE_NOTIFICATIONS=false

# ================================
# PLATFORM-SPECIFIC CONFIGURATION (Windows)
# ================================

# Windows Service Settings
WINDOWS_SERVICE_NAME=OPTradingPlatform
WINDOWS_SERVICE_DESCRIPTION=OP Trading Platform Service
WINDOWS_LOG_EVENT_SOURCE=OPTrading

# Windows-specific paths (use forward slashes or double backslashes)
WINDOWS_DATA_PATH=C:/OP-Trading/data
WINDOWS_LOG_PATH=C:/OP-Trading/logs

# ================================
# MISCELLANEOUS SETTINGS
# ================================

# Timezone and Localization
# Timezone: Application timezone (Indian markets)
TIMEZONE=Asia/Kolkata

# Date/time formats
DATE_FORMAT=%Y-%m-%d
TIME_FORMAT=%H:%M:%S
DATETIME_FORMAT=%Y-%m-%d %H:%M:%S

# Locale: Number and currency formatting
LOCALE=en_IN
CURRENCY=INR
NUMBER_FORMAT=en_IN

# Cache Settings
# Application-level caching (separate from Redis)
ENABLE_CACHING=true

# Cache TTL: How long to cache data (seconds)
CACHE_TTL=3600

# Max cache size: Maximum number of cached items
CACHE_MAX_SIZE=1000

# Cleanup: How often to clean expired cache entries (seconds)
CACHE_CLEANUP_INTERVAL=300

# Rate Limiting (Application Level)
# Global rate limit: Requests per minute for entire application
GLOBAL_RATE_LIMIT=1000

# Per-user limit: Requests per minute per authenticated user
PER_USER_RATE_LIMIT=100

# Window: Rate limit time window (seconds)
RATE_LIMIT_WINDOW=60

# Storage: Where to store rate limit counters
RATE_LIMIT_STORAGE=redis

# Session Management
# Session timeout: How long sessions remain active (seconds)
SESSION_TIMEOUT=3600

# Cleanup: How often to clean expired sessions (seconds)
SESSION_CLEANUP_INTERVAL=300

# Max concurrent: Maximum concurrent sessions per user
MAX_CONCURRENT_SESSIONS=10

# ================================
# CUSTOM SETTINGS
# ================================

# Application Metadata
APPLICATION_NAME=OP Trading Platform
APPLICATION_VERSION=1.0.0
APPLICATION_AUTHOR=Your Name
APPLICATION_EMAIL=your.email@domain.com
APPLICATION_URL=https://your-domain.com
APPLICATION_LICENSE=MIT

# Custom application settings (add your own)
CUSTOM_SETTING_1=value1
CUSTOM_SETTING_2=value2

# ================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# ================================

# Development Mode Overrides (uncomment for development)
# DEBUG=true
# LOG_LEVEL=DEBUG
# MOCK_BROKER_API=true
# API_RELOAD=true
# COMPRESSION_ENABLED=false

# Production Mode Overrides (uncomment for production)
# DEBUG=false
# LOG_LEVEL=INFO
# SECURITY_ENABLED=true
# ENABLE_HTTPS=true
# COMPRESSION_ENABLED=true

# ================================
# HOW TO USE THIS FILE
# ================================

# 1. Copy this file to .env in your project root
# 2. Update values marked with "your_" prefix
# 3. Set DEPLOYMENT_MODE to your target environment
# 4. Review and adjust performance settings for your system
# 5. Test configuration with: python -c "from shared.config.settings import get_settings; print('Config loaded successfully')"

# ================================
# CONFIGURATION VALIDATION
# ================================

# The application will validate these settings on startup
# Invalid configurations will cause startup to fail with descriptive error messages
# Run setup script to validate configuration: python setup.py validate-config